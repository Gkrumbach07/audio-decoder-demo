{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Service Testing\n",
    "In this notebook, we will setup a Kafka consumer and producer, and we will setup a few pre-trained models that will interact with our streamed data.\n",
    "\n",
    "This use case illustrates how we can proccess transcripts of live calls in realtime. We will use some simple sentiment analysis and POS tagging to catagorize which calls are going well and which calls are going poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaProducer\n",
    "import nltk\n",
    "import flair\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Setup\n",
    "This notebook needs to both receive data and send out data, therefore we will need two Kafka topics. Our `from_topic` is the in the form of small snippets of text. We will want to produce to the `to_topic` in the form of a json object of our model output.\n",
    "\n",
    "We define our Kafka producer and consumer bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = 'odh-message-bus-kafka-bootstrap.opf-kafka.svc:9092'\n",
    "from_topic = 'audio-decoder.decoded-speech'\n",
    "to_topic = 'audio-decoder.sentiment-text'\n",
    "\n",
    "consumer = KafkaConsumer(from_topic, bootstrap_servers=brokers, group_id=\"default\")\n",
    "producer = KafkaProducer(bootstrap_servers=brokers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "Here is where we would either build, import, or in our case download our models. We will be using a pre-trianed model from [Flair](https://github.com/flairNLP/flair) for sentiment analsysis. We will also be using [NLTK](http://www.nltk.org/) for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming the Data\n",
    "Now we can start consuming and proccesing the data. To do this, we can itterate over the consumer and proccess each message individually. The message recived includes both an id value and the decoded text. The id is used to differentiate between different calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose to specify a `consumer_id` to identify which call chunks your jupyter notebook proccesed. The beauty of using Kafka to consume, proccess, and produce data is that multiple consumers (notebbok images) can proccess the incoming data together and Kafka splits up the tasks automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the consumer_id to any string\n",
    "consumer_id = \"DEFAULT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in consumer:\n",
    "    if msg.value is not None:\n",
    "        # first we will load in the json object\n",
    "        obj_in = json.loads(msg.value.decode('utf-8'))\n",
    "        \n",
    "        if obj_in[\"sentence\"] == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Using flair, we create a sentence and predict its sentiment.\n",
    "        s = flair.data.Sentence(obj_in[\"sentence\"])\n",
    "        flair_sentiment.predict(s)\n",
    "        \n",
    "        # Using NLTK, we tokenize the sentence and extract only the nouns\n",
    "        text = nltk.word_tokenize(obj_in[\"sentence\"])\n",
    "        tokens = nltk.pos_tag(text)\n",
    "        nouns = []\n",
    "        for pair in tokens:\n",
    "            if pair[1][:2] == 'NN':\n",
    "                nouns.append(pair[0])\n",
    "        \n",
    "        # We complile our model outputs into an object with an ID.\n",
    "        # We use an ID to track which call this text came from\n",
    "        data = {\n",
    "            \"sentence\": obj_in[\"sentence\"],\n",
    "            \"quality\": s.labels[0].value,\n",
    "            \"nouns\": nouns,\n",
    "            \"id\": obj_in[\"id\"],\n",
    "            \"consumer\": consumer_id\n",
    "        }\n",
    "        \n",
    "        print(data[\"id\"])\n",
    "        print(data[\"sentence\"])\n",
    "        print(s.labels[0])\n",
    "        print(data[\"nouns\"])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Now we can send this data out to our to_topic, so it\n",
    "        # can be recived by our web application\n",
    "        producer.send(to_topic, json.dumps(data).encode('utf-8'))\n",
    "\n",
    "print('exiting')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
